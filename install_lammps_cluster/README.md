# ðŸ§ª Installing LAMMPS on Cluster Documentation

This folder documents how to install **LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator) for molecular dynamics simulations.

---

### 1. Folder Content

Navigate to personal storage in the cluster (so that **LAMMPS installation does not take up memory in your user directory)

```
cd /storage/eng/username
```

---

### 2. Check pre-requisits

```
module spider x

```

- CMake/3.29.3
- GCC/13.3.0
- OpenMPI/5.0.3
- Python/3.12.3
- LAMMPS/23June2022-kokkos

---
### 3. Load relevant modules

```bash
module purge
module load GCC
module load OpenMPI
module load Python
module load CMake
```

And check using:

```
module list
```

Save using:
```
module save lammps
```

Reload using:
```
module restore lammps
```


---

### 4. Download LAMMPS

- Git:

```
git clone -b release https://github.com/lammps/lammps.git lammps
```

### (Probably not: download LAMMPS)

- Static Linux, go to: https://download.lammps.org/static/

- Ubuntu and Debian Linux:
```
sudo apt-get install lammps
```

- Fedora Linux
```
dnf install lammps-openmpi
```

- OpenSuse Linux
```
zypper install lammps
```

- Gentoo Linux
```
emerge --ask lammps
```

- Archlinux build-script
```
git clone https://aur.archlinux.org/lammps.git
```

---

### 4. Navigate directory

```
cd lammps
mkdir build
cd build
```
---

### 5. Build LAMMPS


Start interactive session
```

salloc --nodes=1 --ntasks=1 --mem-per-cpu=3988 --cpus-per-task=8 --partition=interactive --time=01:00:00

```

Compile
```
module purge
module restore lammps
cd /storage/eng/esrwwn/lammps/build

# Run cmake and make
cmake ../cmake \
  -D CMAKE_INSTALL_PREFIX=$HOME/lammps-install \
  -D BUILD_MPI=on \
  -D BUILD_OMP=on \
  -D PKG_MOLECULE=on \
  -D PKG_KSPACE=on \
  -D PKG_RIGID=on \
  -D PKG_USER-REAXC=on \
  -D LAMMPS_EXCEPTIONS=on \
  -D BUILD_SHARED_LIBS=on \
  -D -D PKG_CLASS2=on

make -j 28
make install
```
```
exit
```
---

6. Run

Sbatch
```
#!/bin/bash
#
# SLURM job submission script generated by MyCluster
#
#SBATCH --nodelist dedicated120
# Job name
#SBATCH -J sim_0010_270p00_p00

# Send status information to this email address.
##SBATCH --mail-user=
# Send me an e-mail when the job has finished.
##SBATCH --mail-type=ALL
# Redirect output stream to this file.
#SBATCH --output sim_0010_270p00_p00.out.%j
# Which project should be charged
##SBATCH -A turbine
# Partition name
#SBATCH -p mnf
# Number of tasks
# Exclusive node use
##SBATCH --exclusive
# Do not requeue job on node failure
##SBATCH --no-requeue
# High performance cpu governor

##SBATCH --cpu-freq=Performance

#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16  #1-28, 8 recommended
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=3988
#SBATCH --time=04:00:00

module purge
module restore lammps
cd /storage/eng/esrwwn/lammps_sim/my_epoxy_higher_density

```

```
sbatch jobscript
squeue -u username
scancel 785030
```


Running on dedicated node

```
ssh esrwwn@dedicated120.csc.warwick.ac.uk
```


Run in parallel
```
mpirun -n 16 /storage/eng/esrwwn/lammps/build/lmp -in my_epoxy_higher_density.in
```
Run normal:
```
/storage/eng/esrwwn/lammps/build/lmp -in my_epoxy_higher_density.in
```


Useful commands
```
ll

top

q

less file.in

```





---


## ðŸ§  Notes

Shortcut to storage in home directory
```
ln -s /storage/eng/esrwwn storage
```


---

## ðŸ”— References

- [LAMMPS Documentation](https://docs.lammps.org/)
